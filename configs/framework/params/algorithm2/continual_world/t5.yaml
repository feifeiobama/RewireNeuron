params:
  optimizer_policy:
    lr: 1e-3

  optimizer_q:
    lr: 1e-3

  optimizer_entropy:
    lr: 1e-3

  reward_scaling: 1.
  policy_update_delay: 1
  target_update_delay: 1

  n_timesteps: 50  ## burn-in timesteps
  batch_size: 128   ## 128,256,512

  init_temperature: 2.7
  target_multiplier: 2.

  clip_grad: 0.
  inner_epochs: ${framework.params.algorithm.params.n_timesteps}
  grad_updates_per_step: 1.
  buffer_size: 1_000_000
  initial_buffer_size: 1_000